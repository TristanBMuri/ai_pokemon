/home/tristan/CodingProjects/ai_pokemon/.venv/lib/python3.13/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
Observation Space Type: <class 'gymnasium.spaces.box.Box'>
Observation Space: Box(0.0, 1.0, (5,), float32)
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Starting training...
Logging to tmp/battler/PPO_6
2025-11-29 17:33:02,705 - Agent_de5a2feb - WARNING - Popup message received: |popup|Due to high load, you are limited to 12 battles and team validations every 3 minutes.
